# ================================
# AutoVoiceCollation 配置文件示例
# ================================
# 使用说明：
# 1. 复制此文件为 .env
# 2. 根据需要修改配置项
# 3. .env 文件不会被提交到 git（已在 .gitignore 中）
# ================================

# ================================
# API Keys（必需）
# ================================
# DeepSeek API Key
DEEPSEEK_API_KEY=Your_DeepSeek_API_Key_Here

# Google Gemini API Key
GEMINI_API_KEY=Your_Google_Gemini_API_Key_Here

# 阿里云 DashScope API Key（用于通义千问）
DASHSCOPE_API_KEY=Your_Aliyun_DashScope_API_Key_Here

# Cerebras API Key
CEREBRAS_API_KEY=Your_Cerebras_API_Key_Here

# ================================
# 目录配置
# ================================
# 输出目录（处理后的文件存放位置）
OUTPUT_DIR=./out

# 下载目录（B站音频下载位置）
DOWNLOAD_DIR=./download

# 临时文件目录
TEMP_DIR=./temp

# 模型缓存目录（留空则使用系统默认缓存目录）
MODEL_DIR=

# 日志目录
LOG_DIR=./logs

# ================================
# 日志配置
# ================================
# 日志级别：DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# 日志文件路径（留空则不写入文件）
LOG_FILE=./logs/AutoVoiceCollation.log

# 是否输出到控制台：true 或 false
LOG_CONSOLE_OUTPUT=true

# 控制台输出是否使用彩色：true 或 false
LOG_COLORED_OUTPUT=true

# 第三方库日志级别：DEBUG, INFO, WARNING, ERROR, CRITICAL
# 建议设置为 WARNING 或 ERROR 以减少噪音
THIRD_PARTY_LOG_LEVEL=ERROR

# ================================
# ASR 模型配置
# ================================
# ASR 模型选择：sense_voice 或 paraformer
ASR_MODEL=paraformer

# ================================
# 输出样式配置
# ================================
# 输出样式：pdf_with_img, img_only, text_only, pdf_only
OUTPUT_STYLE=pdf_only

# 是否输出 zip 压缩包：true 或 false
ZIP_OUTPUT_ENABLED=false

# Web UI 中是否默认仅返回纯文本（JSON）结果：true 或 false
# 当设置为 true 时，Web UI 各处理页的“仅返回文本(JSON)”复选框将默认被勾选。
TEXT_ONLY_DEFAULT=false

# ================================
# LLM 配置
# ================================
# LLM 服务选择
# 可选值：
# - gemini-2.0-flash
# - deepseek-chat
# - deepseek-reasoner
# - qwen3-max
# - qwen3-plus
# - Cerebras:Qwen-3-32B
# - Cerebras:Qwen-3-235B-Instruct
# - Cerebras:Qwen-3-235B-Thinking
# - local:Qwen/Qwen2.5-1.5B-Instruct
LLM_SERVER=Cerebras:Qwen-3-235B-Instruct

# LLM 温度（0.0-2.0，越高越随机）
LLM_TEMPERATURE=0.1

# LLM 最大 tokens
LLM_MAX_TOKENS=6000

# LLM Top-p（0.0-1.0）
LLM_TOP_P=0.95

# LLM Top-k
LLM_TOP_K=64

# 文本分段长度（每段文本的最大字符数）
SPLIT_LIMIT=1000

# 是否使用异步处理：true 或 false
ASYNC_FLAG=true

# ================================
# 摘要生成配置
# ================================
# 摘要 LLM 服务
SUMMARY_LLM_SERVER=Cerebras:Qwen-3-235B-Thinking

# 摘要 LLM 温度
SUMMARY_LLM_TEMPERATURE=1

# 摘要 LLM 最大 tokens
SUMMARY_LLM_MAX_TOKENS=8192

# ================================
# 功能开关
# ================================
# 是否禁用 LLM 润色：true 或 false
DISABLE_LLM_POLISH=false

# 是否禁用 LLM 摘要：true 或 false
DISABLE_LLM_SUMMARY=false

# 是否启用本地 LLM：true 或 false
LOCAL_LLM_ENABLED=false

# 调试模式：true 或 false
DEBUG_FLAG=false

# ================================
# Web 服务器配置
# ================================
# Web 服务器端口（留空则不启动 Web 服务）
WEB_SERVER_PORT=
