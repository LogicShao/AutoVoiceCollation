# ================================
# AutoVoiceCollation 配置文件示例
# ================================
# 使用说明：
# 1. 复制此文件为 .env
# 2. 根据需要修改配置项
# 3. .env 文件不会被提交到 git（已在 .gitignore 中）
# ================================

# ================================
# API Keys（必需）
# ================================
# DeepSeek API Key
DEEPSEEK_API_KEY=Your_DeepSeek_API_Key_Here

# Google Gemini API Key
GEMINI_API_KEY=Your_Google_Gemini_API_Key_Here

# 阿里云 DashScope API Key（用于通义千问）
DASHSCOPE_API_KEY=Your_Aliyun_DashScope_API_Key_Here

# Cerebras API Key
CEREBRAS_API_KEY=Your_Cerebras_API_Key_Here

# ================================
# 目录配置
# ================================
# 输出目录（处理后的文件存放位置）
OUTPUT_DIR=./out

# 下载目录（B站音频下载位置）
DOWNLOAD_DIR=./download

# 临时文件目录
TEMP_DIR=./temp

# 模型缓存目录（留空则使用系统默认缓存目录）
MODEL_DIR=

# 日志目录
LOG_DIR=./logs

# ================================
# 日志配置
# ================================
# 日志级别：DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# 日志文件路径（留空则不写入文件）
LOG_FILE=./logs/AutoVoiceCollation.log

# 是否输出到控制台：true 或 false
LOG_CONSOLE_OUTPUT=true

# 控制台输出是否使用彩色：true 或 false
LOG_COLORED_OUTPUT=true

# 第三方库日志级别：DEBUG, INFO, WARNING, ERROR, CRITICAL
# 建议设置为 WARNING 或 ERROR 以减少噪音
THIRD_PARTY_LOG_LEVEL=ERROR

# ================================
# ASR 模型配置
# ================================
# ASR 模型选择：sense_voice / paraformer / whisper_cpp
ASR_MODEL=paraformer

# ================================
# whisper.cpp 配置（可选）
# 仅当 ASR_MODEL=whisper_cpp 时生效
# ================================
# whisper.cpp 可执行文件（建议使用 whisper-cli.exe）
WHISPER_CPP_BIN=./assets/whisper.cpp/whisper-cli.exe

# whisper.cpp ggml 模型文件（你当前使用：ggml-medium-q5_0.bin）
WHISPER_CPP_MODEL=./assets/models/ggml-medium-q5_0.bin

# 语言：auto/zh/en/...
WHISPER_CPP_LANGUAGE=auto

# 线程数：建议设置为 CPU 物理核数或略低
WHISPER_CPP_THREADS=4

# VAD（可选，需要 VAD 模型）
# 开启后会使用 VAD 自动切分语音段落（常用 Silero VAD：ggml-silero-*.bin）
WHISPER_CPP_VAD=false
# WHISPER_CPP_VAD_MODEL=./assets/models/ggml-silero-v6.2.0.bin

# 额外参数（原样传给 whisper-cli.exe）
# 常用：--no-gpu
WHISPER_CPP_EXTRA_ARGS=--no-gpu

# ================================
# 设备与推理配置
# ================================
# 设备选择：auto, cpu, cuda, cuda:0, cuda:1 等
# - auto: 自动检测，优先使用 GPU（推荐）
# - cpu: 强制使用 CPU
# - cuda 或 cuda:0: 使用第一个 CUDA 设备
# - cuda:1, cuda:2 等: 使用指定的 CUDA 设备
DEVICE=auto

# 是否启用 ONNX 推理：true 或 false
# 注意：需要先安装 onnxruntime 或 onnxruntime-gpu
# ONNX 推理可能提供更好的性能和兼容性
# 但并非所有 FunASR 模型都支持 ONNX
USE_ONNX=false

# ONNX 执行提供者（留空则自动选择）
# 可选值（逗号分隔）：
# - CUDAExecutionProvider: CUDA GPU 加速
# - CPUExecutionProvider: CPU 执行
# - TensorrtExecutionProvider: TensorRT 加速
# 示例：ONNX_PROVIDERS=CUDAExecutionProvider,CPUExecutionProvider
ONNX_PROVIDERS=

# ================================
# 输出样式配置
# ================================
# 输出样式：pdf_with_img, img_only, text_only, pdf_only
OUTPUT_STYLE=pdf_only

# ================================
# PDF 字体配置
# ================================
# PDF 主字体路径（可选，支持相对路径）
# 示例：PDF_FONT_PATH=./assets/fonts/NotoSansCJK-Regular.ttc
PDF_FONT_PATH=

# PDF 拉丁字体路径（可选，用于修复 Žižek 等字符显示）
# 示例：PDF_LATIN_FONT_PATH=./assets/fonts/DejaVuSans.ttf
PDF_LATIN_FONT_PATH=

# 是否输出 zip 压缩包：true 或 false
ZIP_OUTPUT_ENABLED=false

# Web UI 中是否默认仅返回纯文本（JSON）结果：true 或 false
# 当设置为 true 时，Web UI 各处理页的“仅返回文本(JSON)”复选框将默认被勾选。
TEXT_ONLY_DEFAULT=false

# ================================
# LLM 配置
# ================================
# LLM 服务选择
# 可选值：
# - gemini-2.0-flash
# - deepseek-chat
# - deepseek-reasoner
# - qwen3-max
# - qwen3-plus
# - Cerebras:Qwen-3-32B
# - Cerebras:Qwen-3-235B-Instruct
# - Cerebras:Qwen-3-235B-Thinking
# - local:Qwen/Qwen2.5-1.5B-Instruct
LLM_SERVER=Cerebras:Qwen-3-235B-Instruct

# LLM 温度（0.0-2.0，越高越随机）
LLM_TEMPERATURE=0.1

# LLM 最大 tokens
LLM_MAX_TOKENS=8000

# LLM Top-p（0.0-1.0）
LLM_TOP_P=0.95

# LLM Top-k
LLM_TOP_K=64

# 文本分段长度（每段文本的最大字符数）
SPLIT_LIMIT=6000

# 是否使用异步处理：true 或 false
ASYNC_FLAG=true

# ================================
# 摘要生成配置
# ================================
# 摘要 LLM 服务
SUMMARY_LLM_SERVER=Cerebras:Qwen-3-235B-Thinking

# 摘要 LLM 温度
SUMMARY_LLM_TEMPERATURE=1

# 摘要 LLM 最大 tokens
SUMMARY_LLM_MAX_TOKENS=8192

# ================================
# 功能开关
# ================================
# 是否禁用 LLM 润色：true 或 false
DISABLE_LLM_POLISH=false

# 是否禁用 LLM 摘要：true 或 false
DISABLE_LLM_SUMMARY=false

# 是否启用本地 LLM：true 或 false
LOCAL_LLM_ENABLED=false

# 调试模式：true 或 false
DEBUG_FLAG=false

# ================================
# Web 服务器配置
# ================================
# Web 服务器端口（留空则不启动 Web 服务）
WEB_SERVER_PORT=
