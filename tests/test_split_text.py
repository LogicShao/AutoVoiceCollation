"""
æ–‡æœ¬åˆ†å‰²æ¨¡å—å•å…ƒæµ‹è¯•
æµ‹è¯•æ–‡æœ¬æŒ‰å¥å­åˆ†å‰²ã€æ™ºèƒ½åˆ†å‰²å’Œ ASR æ–‡æœ¬æ¸…ç†åŠŸèƒ½
"""

import re

import pytest

from src.text_arrangement.split_text import (
    clean_asr_text,
    is_chinese,
    smart_split,
    split_text_by_sentences,
)


class TestSplitTextBySentences:
    """æµ‹è¯•æŒ‰å¥å­åˆ†å‰²æ–‡æœ¬"""

    def test_split_simple_text(self):
        """æµ‹è¯•ç®€å•æ–‡æœ¬åˆ†å‰²"""
        text = "è¿™æ˜¯ç¬¬ä¸€å¥ã€‚è¿™æ˜¯ç¬¬äºŒå¥ã€‚è¿™æ˜¯ç¬¬ä¸‰å¥ã€‚"
        result = split_text_by_sentences(text, split_len=20)

        assert len(result) > 0
        assert all(isinstance(chunk, str) for chunk in result)

    def test_split_with_multiple_punctuation(self):
        """æµ‹è¯•å¤šç§æ ‡ç‚¹ç¬¦å·"""
        text = "è¿™æ˜¯é—®é¢˜å—ï¼Ÿè¿™æ˜¯æ„Ÿå¹å¥ï¼è¿™æ˜¯æ™®é€šå¥å­ã€‚"
        result = split_text_by_sentences(text, split_len=50)

        assert len(result) == 1
        assert "è¿™æ˜¯é—®é¢˜å—ï¼Ÿ" in result[0]

    def test_split_exceeds_limit(self):
        """æµ‹è¯•è¶…è¿‡é•¿åº¦é™åˆ¶çš„æ–‡æœ¬"""
        text = "è¿™æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„å¥å­ã€‚" * 10
        result = split_text_by_sentences(text, split_len=30)

        assert len(result) > 1
        for chunk in result:
            # æ¯ä¸ªå—çš„é•¿åº¦ä¸åº”ä¸¥é‡è¶…è¿‡é™åˆ¶ï¼ˆå…è®¸å•ä¸ªå¥å­è¶…é™ï¼‰
            pass  # å‡½æ•°è®¾è®¡å…è®¸å¥å­æœ¬èº«è¶…è¿‡é™åˆ¶

    def test_split_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬"""
        text = ""
        result = split_text_by_sentences(text, split_len=100)

        # ç©ºæ–‡æœ¬åº”è¯¥è¿”å›ç©ºåˆ—è¡¨æˆ–åªåŒ…å«ç©ºå­—ç¬¦ä¸²
        assert len(result) == 0 or (len(result) == 1 and result[0] == "")

    def test_split_single_sentence(self):
        """æµ‹è¯•å•ä¸ªå¥å­"""
        text = "è¿™æ˜¯å”¯ä¸€çš„ä¸€å¥è¯ã€‚"
        result = split_text_by_sentences(text, split_len=100)

        assert len(result) == 1
        assert result[0] == "è¿™æ˜¯å”¯ä¸€çš„ä¸€å¥è¯ã€‚"

    def test_split_preserves_trailing_text_without_terminal_punctuation(self):
        """æµ‹è¯•æœ«å°¾æ²¡æœ‰æ ‡ç‚¹æ—¶ä¸ä¸¢å¤±å°¾éƒ¨å†…å®¹"""
        text = "ç¬¬ä¸€å¥ã€‚ç¬¬äºŒå¥æ²¡æœ‰å¥å·"
        result = split_text_by_sentences(text, split_len=100)
        merged = "".join(result)

        assert "ç¬¬ä¸€å¥ã€‚" in merged
        assert "ç¬¬äºŒå¥æ²¡æœ‰å¥å·" in merged

    def test_split_no_punctuation(self):
        """æµ‹è¯•æ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬"""
        text = "è¿™æ˜¯ä¸€æ®µæ²¡æœ‰æ ‡ç‚¹ç¬¦å·çš„æ–‡æœ¬" * 3
        result = split_text_by_sentences(text, split_len=10)

        assert len(result) > 0
        assert all(chunk.strip() for chunk in result)
        assert all(len(chunk) <= 10 for chunk in result)

        # å»æ‰ç©ºç™½åå†…å®¹åº”ä¿æŒä¸€è‡´ï¼ˆsplit_text_by_sentences ä¼š strip chunkï¼‰
        norm = lambda s: re.sub(r"\s+", "", s)  # noqa: E731
        assert norm("".join(result)) == norm(text)

    def test_split_mixed_punctuation(self):
        """æµ‹è¯•ä¸­è‹±æ–‡æ··åˆæ ‡ç‚¹"""
        text = "This is English. è¿™æ˜¯ä¸­æ–‡ã€‚Another sentence! å¦ä¸€å¥ï¼Ÿ"
        result = split_text_by_sentences(text, split_len=100)

        assert len(result) == 1
        assert "English" in result[0]
        assert "ä¸­æ–‡" in result[0]

    def test_split_very_small_limit(self):
        """æµ‹è¯•éå¸¸å°çš„åˆ†å‰²é™åˆ¶"""
        text = "ç¬¬ä¸€å¥ã€‚ç¬¬äºŒå¥ã€‚ç¬¬ä¸‰å¥ã€‚"
        result = split_text_by_sentences(text, split_len=5)

        # æ¯ä¸ªå¥å­éƒ½åº”è¯¥è¢«å•ç‹¬åˆ†å‰²
        assert len(result) > 1

    def test_split_consecutive_punctuation(self):
        """æµ‹è¯•è¿ç»­æ ‡ç‚¹ç¬¦å·"""
        text = "çœŸçš„å—ï¼Ÿï¼Ÿï¼ï¼æ˜¯çš„ã€‚ã€‚"
        result = split_text_by_sentences(text, split_len=100)

        assert len(result) >= 1

    def test_split_preserves_content(self):
        """æµ‹è¯•åˆ†å‰²åå†…å®¹å®Œæ•´æ€§"""
        text = "ç¬¬ä¸€å¥ã€‚ç¬¬äºŒå¥ã€‚ç¬¬ä¸‰å¥ã€‚ç¬¬å››å¥ã€‚"
        result = split_text_by_sentences(text, split_len=15)

        # åˆå¹¶æ‰€æœ‰åˆ†å—ï¼Œåº”è¯¥åŒ…å«åŸå§‹å†…å®¹çš„æ‰€æœ‰å¥å­
        merged = "".join(result)
        assert "ç¬¬ä¸€å¥" in merged
        assert "ç¬¬äºŒå¥" in merged
        assert "ç¬¬ä¸‰å¥" in merged
        assert "ç¬¬å››å¥" in merged

    def test_split_with_english_punctuation(self):
        """æµ‹è¯•è‹±æ–‡æ ‡ç‚¹ç¬¦å·"""
        text = "First sentence. Second sentence! Third sentence?"
        result = split_text_by_sentences(text, split_len=100)

        assert len(result) == 1
        assert "First" in result[0]
        assert "Third" in result[0]

    def test_split_long_sentences(self):
        """æµ‹è¯•å¾ˆé•¿çš„å•ä¸ªå¥å­"""
        text = "è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸éå¸¸éå¸¸é•¿çš„å¥å­ï¼ŒåŒ…å«äº†å¾ˆå¤šå¾ˆå¤šçš„å†…å®¹ã€‚"
        result = split_text_by_sentences(text, split_len=10)

        # å³ä½¿è¶…è¿‡é™åˆ¶ï¼Œå•ä¸ªå¥å­ä¹Ÿåº”è¯¥ä¿æŒå®Œæ•´
        # æ³¨æ„ï¼šå‡½æ•°å¯èƒ½ä¼šåˆ†å‰²å¥å­ï¼Œæ‰€ä»¥åªéªŒè¯ç»“æœä¸ä¸ºç©º
        assert len(result) >= 1
        # éªŒè¯å†…å®¹å®Œæ•´æ€§
        merged = "".join(result)
        assert "éå¸¸" in merged


class TestCleanAsrText:
    """æµ‹è¯• ASR æ–‡æœ¬æ¸…ç†"""

    def test_clean_simple_tags(self):
        """æµ‹è¯•æ¸…ç†ç®€å•æ ‡ç­¾"""
        text = "è¿™æ˜¯æ–‡æœ¬<|tag1|>å†…å®¹<|tag2|>ç»“æŸ"
        result = clean_asr_text(text)

        assert "<|tag1|>" not in result
        assert "<|tag2|>" not in result
        assert "è¿™æ˜¯æ–‡æœ¬" in result
        assert "å†…å®¹" in result
        assert "ç»“æŸ" in result

    def test_clean_no_tags(self):
        """æµ‹è¯•æ²¡æœ‰æ ‡ç­¾çš„æ–‡æœ¬"""
        text = "è¿™æ˜¯ä¸€æ®µæ²¡æœ‰æ ‡ç­¾çš„æ­£å¸¸æ–‡æœ¬"
        result = clean_asr_text(text)

        assert result == text

    def test_clean_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬"""
        text = ""
        result = clean_asr_text(text)

        assert result == ""

    def test_clean_only_tags(self):
        """æµ‹è¯•åªæœ‰æ ‡ç­¾çš„æ–‡æœ¬"""
        text = "<|tag1|><|tag2|><|tag3|>"
        result = clean_asr_text(text)

        assert result == ""

    def test_clean_nested_tags(self):
        """æµ‹è¯•åµŒå¥—æ ‡ç­¾ï¼ˆå®é™…ä¸åº”è¯¥å‡ºç°ï¼Œä½†æµ‹è¯•é²æ£’æ€§ï¼‰"""
        text = "æ–‡æœ¬<|outer<|inner|>|>ç»“æŸ"
        result = clean_asr_text(text)

        # æ­£åˆ™è¡¨è¾¾å¼åº”è¯¥åŒ¹é…æœ€çŸ­çš„æ ‡ç­¾
        assert "<|" not in result or "|>" not in result

    def test_clean_special_characters_in_tags(self):
        """æµ‹è¯•æ ‡ç­¾å†…åŒ…å«ç‰¹æ®Šå­—ç¬¦"""
        text = "æ–‡æœ¬<|tag-1_2.3|>å†…å®¹<|tag@#$|>ç»“æŸ"
        result = clean_asr_text(text)

        assert "<|tag-1_2.3|>" not in result
        assert "<|tag@#$|>" not in result

    def test_clean_multiple_consecutive_tags(self):
        """æµ‹è¯•è¿ç»­å¤šä¸ªæ ‡ç­¾"""
        text = "å¼€å§‹<|tag1|><|tag2|><|tag3|>ä¸­é—´<|tag4|>ç»“æŸ"
        result = clean_asr_text(text)

        assert "<|" not in result
        assert "|>" not in result
        assert "å¼€å§‹" in result
        assert "ä¸­é—´" in result
        assert "ç»“æŸ" in result

    def test_clean_incomplete_tags(self):
        """æµ‹è¯•ä¸å®Œæ•´çš„æ ‡ç­¾"""
        text = "æ–‡æœ¬<|incomplete å†…å®¹ |>incomplete> ç»“æŸ"
        result = clean_asr_text(text)

        # åªæœ‰å®Œæ•´çš„ <|...|> æ ¼å¼æ‰ä¼šè¢«ç§»é™¤
        assert "<|incomplete å†…å®¹ |>" not in result


class TestIsChinese:
    """æµ‹è¯•ä¸­æ–‡å­—ç¬¦åˆ¤æ–­"""

    def test_is_chinese_true(self):
        """æµ‹è¯•ä¸­æ–‡å­—ç¬¦"""
        assert is_chinese("ä¸­") is True
        assert is_chinese("æ–‡") is True
        assert is_chinese("å­—") is True
        assert is_chinese("å¥½") is True

    def test_is_chinese_false(self):
        """æµ‹è¯•éä¸­æ–‡å­—ç¬¦"""
        assert is_chinese("a") is False
        assert is_chinese("A") is False
        assert is_chinese("1") is False
        assert is_chinese(" ") is False
        assert is_chinese("!") is False

    def test_is_chinese_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œ Unicode å­—ç¬¦"""
        # Unicode ä¸­æ–‡èŒƒå›´: \u4e00 - \u9fff
        assert is_chinese("\u4e00") is True  # æœ€å°ä¸­æ–‡å­—ç¬¦
        assert is_chinese("\u9fff") is True  # æœ€å¤§ä¸­æ–‡å­—ç¬¦
        assert is_chinese("\u4dff") is False  # å°äºèŒƒå›´
        assert is_chinese("\ua000") is False  # å¤§äºèŒƒå›´

    def test_is_chinese_special_symbols(self):
        """æµ‹è¯•ç‰¹æ®Šç¬¦å·"""
        assert is_chinese("ã€‚") is False  # ä¸­æ–‡å¥å·
        assert is_chinese("ï¼Œ") is False  # ä¸­æ–‡é€—å·
        assert is_chinese("ï¼Ÿ") is False  # ä¸­æ–‡é—®å·


class TestSmartSplit:
    """æµ‹è¯•æ™ºèƒ½åˆ†å‰²åŠŸèƒ½"""

    def test_smart_split_simple(self):
        """æµ‹è¯•ç®€å•æ™ºèƒ½åˆ†å‰²"""
        text = "è¿™æ˜¯ç¬¬ä¸€æ®µ è¿™æ˜¯ç¬¬äºŒæ®µ è¿™æ˜¯ç¬¬ä¸‰æ®µ"
        result = smart_split(text, split_len=15)

        assert len(result) > 0
        assert all(isinstance(chunk, str) for chunk in result)

    def test_smart_split_chinese_text(self):
        """æµ‹è¯•çº¯ä¸­æ–‡æ–‡æœ¬æ™ºèƒ½åˆ†å‰²"""
        text = "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„ä¸­æ–‡æ–‡æœ¬éœ€è¦è¿›è¡Œæ™ºèƒ½åˆ†å‰²å¤„ç†"
        result = smart_split(text, split_len=10)

        assert len(result) > 1
        for chunk in result:
            # æ¯ä¸ªå—çš„é•¿åº¦åº”è¯¥æ¥è¿‘é™åˆ¶
            assert len(chunk) <= 10 + 5  # å…è®¸ä¸€å®šè¯¯å·®

    def test_smart_split_with_spaces(self):
        """æµ‹è¯•åŒ…å«ç©ºæ ¼çš„æ–‡æœ¬"""
        text = "This is English text with spaces that needs splitting"
        result = smart_split(text, split_len=20)

        assert len(result) > 1
        # åº”è¯¥ä¼˜å…ˆåœ¨ç©ºæ ¼å¤„åˆ†å‰²
        for chunk in result:
            # æ£€æŸ¥åˆ†å‰²ç‚¹æ˜¯å¦åˆç†
            pass

    def test_smart_split_empty_text(self):
        """æµ‹è¯•ç©ºæ–‡æœ¬"""
        text = ""
        result = smart_split(text, split_len=10)

        # ç©ºæ–‡æœ¬åº”è¯¥è¿”å›ç©ºåˆ—è¡¨æˆ–åªåŒ…å«ç©ºå­—ç¬¦ä¸²
        assert len(result) == 0 or (len(result) == 1 and result[0] == "")

    def test_smart_split_exact_length(self):
        """æµ‹è¯•æ–‡æœ¬é•¿åº¦æ­£å¥½ç­‰äºåˆ†å‰²é™åˆ¶"""
        text = "åä¸ªä¸­æ–‡å­—ç¬¦æ­£å¥½"
        result = smart_split(text, split_len=10)

        assert len(result) >= 1

    def test_smart_split_very_long_text(self):
        """æµ‹è¯•å¾ˆé•¿çš„æ–‡æœ¬"""
        text = "è¿™æ˜¯ä¸€ä¸ªéå¸¸é•¿çš„æ–‡æœ¬" * 20
        result = smart_split(text, split_len=30)

        assert len(result) > 1
        # éªŒè¯æ²¡æœ‰å†…å®¹ä¸¢å¤±
        merged = "".join(result)
        assert len(merged.replace(" ", "")) >= len(text.replace(" ", "")) - 10  # å…è®¸ç©ºæ ¼å¤„ç†çš„å·®å¼‚

    def test_smart_split_no_break_points(self):
        """æµ‹è¯•æ²¡æœ‰åˆé€‚åˆ†å‰²ç‚¹çš„æ–‡æœ¬"""
        text = "a" * 50  # 50 ä¸ªè¿ç»­çš„ 'a'
        result = smart_split(text, split_len=20)

        # åº”è¯¥è¿›è¡Œç¡¬åˆ†å‰²
        assert len(result) > 1

    def test_smart_split_mixed_language(self):
        """æµ‹è¯•ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬"""
        text = "è¿™æ˜¯ä¸­æ–‡ This is English è¿™åˆæ˜¯ä¸­æ–‡ More English"
        result = smart_split(text, split_len=20)

        assert len(result) >= 1
        # éªŒè¯å†…å®¹å®Œæ•´æ€§
        merged = "".join(result)
        assert "ä¸­æ–‡" in merged
        assert "English" in merged

    def test_smart_split_preserves_order(self):
        """æµ‹è¯•åˆ†å‰²ä¿æŒé¡ºåº"""
        text = "ç¬¬ä¸€éƒ¨åˆ† ç¬¬äºŒéƒ¨åˆ† ç¬¬ä¸‰éƒ¨åˆ† ç¬¬å››éƒ¨åˆ†"
        result = smart_split(text, split_len=15)

        # åˆå¹¶ååº”è¯¥ä¿æŒåŸå§‹é¡ºåº
        merged = " ".join(result)
        assert merged.index("ç¬¬ä¸€") < merged.index("ç¬¬äºŒ")
        assert merged.index("ç¬¬äºŒ") < merged.index("ç¬¬ä¸‰")
        assert merged.index("ç¬¬ä¸‰") < merged.index("ç¬¬å››")

    def test_smart_split_single_word(self):
        """æµ‹è¯•å•ä¸ªå•è¯"""
        text = "å•è¯"
        result = smart_split(text, split_len=10)

        assert len(result) == 1
        assert result[0] == "å•è¯"

    def test_smart_split_consecutive_spaces(self):
        """æµ‹è¯•è¿ç»­ç©ºæ ¼"""
        text = "æ–‡æœ¬    åŒ…å«    å¤šä¸ª    ç©ºæ ¼"
        result = smart_split(text, split_len=10)

        assert len(result) >= 1
        # ç©ºæ ¼åº”è¯¥è¢«æ­£ç¡®å¤„ç†


class TestEdgeCasesAndRobustness:
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µå’Œé²æ£’æ€§"""

    def test_split_text_very_large_limit(self):
        """æµ‹è¯•éå¸¸å¤§çš„åˆ†å‰²é™åˆ¶"""
        text = "è¿™æ˜¯æ–‡æœ¬ã€‚"
        result = split_text_by_sentences(text, split_len=10000)

        assert len(result) == 1

    def test_split_text_zero_limit(self):
        """æµ‹è¯•é›¶åˆ†å‰²é™åˆ¶"""
        text = "è¿™æ˜¯æ–‡æœ¬ã€‚"
        # å¯èƒ½ä¼šå¯¼è‡´å¼‚å¸¸æˆ–ç‰¹æ®Šè¡Œä¸º
        result = split_text_by_sentences(text, split_len=0)
        # å‡½æ•°åº”è¯¥å¤„ç†è¿™ç§æƒ…å†µ

    def test_split_text_negative_limit(self):
        """æµ‹è¯•è´Ÿæ•°åˆ†å‰²é™åˆ¶"""
        text = "è¿™æ˜¯æ–‡æœ¬ã€‚"
        # å¯èƒ½ä¼šå¯¼è‡´å¼‚å¸¸æˆ–ç‰¹æ®Šè¡Œä¸º
        result = split_text_by_sentences(text, split_len=-10)
        # å‡½æ•°åº”è¯¥å¤„ç†è¿™ç§æƒ…å†µ

    def test_clean_asr_very_long_text(self):
        """æµ‹è¯•æ¸…ç†éå¸¸é•¿çš„æ–‡æœ¬"""
        text = "æ–‡æœ¬" + "<|tag|>" * 1000 + "ç»“æŸ"
        result = clean_asr_text(text)

        assert "<|tag|>" not in result
        assert "æ–‡æœ¬" in result
        assert "ç»“æŸ" in result

    def test_smart_split_unicode_characters(self):
        """æµ‹è¯•åŒ…å« Unicode å­—ç¬¦çš„æ–‡æœ¬"""
        text = "æ–‡æœ¬ ğŸ˜€ emoji ğŸ‰ ç¬¦å· â˜…"
        result = smart_split(text, split_len=10)

        assert len(result) >= 1

    def test_is_chinese_emoji(self):
        """æµ‹è¯• emoji ä¸æ˜¯ä¸­æ–‡"""
        assert is_chinese("ğŸ˜€") is False
        assert is_chinese("ğŸ‰") is False


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
